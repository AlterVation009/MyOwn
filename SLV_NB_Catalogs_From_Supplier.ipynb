{"cells":[{"cell_type":"markdown","source":["### This notebook runs the dimensional load for all dims from sybase."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cde049cb-73f7-4d1b-87a8-fe01129fed73"},{"cell_type":"code","source":["%run SLV_NB_Functions"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"f12487cd-84ed-4c68-b0ab-2929d9eb9ae9","normalized_state":"finished","queued_time":"2025-04-03T23:56:43.9906493Z","session_start_time":null,"execution_start_time":"2025-04-03T23:56:50.169549Z","execution_finish_time":"2025-04-03T23:56:50.2283137Z","parent_msg_id":"84173872-4fcc-4d12-a03f-c8ed50015379"},"text/plain":"StatementMeta(, f12487cd-84ed-4c68-b0ab-2929d9eb9ae9, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: In reference DEV_SLV_NB_Functions run, the default lakehouse of the main notebook will be the effective default lakehouse of the session during reference run. Recommend using absolute path to read/write lakehouse in the referenced notebooks.\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee0cc52d-cfa6-4e58-904e-f3a36e6b5c86"},{"cell_type":"markdown","source":["# Dim Supplier Section"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ffdcd43-492f-4fc8-b3c8-6ea5167cb0f1"},{"cell_type":"code","source":["fproveedores_df = spark.read.parquet(get_latest_file('fproveedores')) ##supplier \n","\n","fproveedores_df.createOrReplaceTempView(\"Supplier\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"d66e89fa-c365-40d5-87bc-3b3d61486efc","normalized_state":"finished","queued_time":"2025-02-27T20:54:38.2459874Z","session_start_time":null,"execution_start_time":"2025-02-27T20:54:38.4288813Z","execution_finish_time":"2025-02-27T20:54:46.6214125Z","parent_msg_id":"86a5fe94-30ce-4e44-ae20-3d9599ff1e5c"},"text/plain":"StatementMeta(, d66e89fa-c365-40d5-87bc-3b3d61486efc, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"72dee03f-f753-4f26-b58b-539d77b5ae11"},{"cell_type":"code","source":[" from pyspark.sql import SparkSession\n"," from pyspark.sql.functions import col, lit, expr, date_format, dayofweek, weekofyear, month, quarter, year, dayofmonth, dayofyear, last_day, when\n","\n","# Define the date range\n","# start_date = \"2015-01-01\"\n","# end_date = \"2050-12-31\"\n","\n"," # Generate a Date Sequence\n"," date_df = spark.sql(f\"SELECT explode(sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day)) as date\")\n","\n","# # Create DimDate attributes\n"," dim_date_df = date_df.select(\n","     col(\"date\").alias(\"full_date\"),\n","     year(\"date\").alias(\"year\"),\n","     quarter(\"date\").alias(\"quarter\"),\n","     month(\"date\").alias(\"month\"),\n","     date_format(\"date\", \"MMMM\").alias(\"month_name\"),\n","     date_format(\"date\", \"E\").alias(\"day_name\"),\n","     dayofmonth(\"date\").alias(\"day_of_month\"),\n","     dayofweek(\"date\").alias(\"day_of_week\"),  # 1=Sunday, 7=Saturday\n","     dayofyear(\"date\").alias(\"day_of_year\"),\n","     weekofyear(\"date\").alias(\"week_of_year\"),\n","     when((dayofweek(\"date\") == 1) | (dayofweek(\"date\") == 7), lit(1)).otherwise(lit(0)).alias(\"is_weekend\"),\n","     expr(\"CASE WHEN month(date) IN (1, 2, 3) THEN 'Q1' WHEN month(date) IN (4, 5, 6) THEN 'Q2' WHEN month(date) IN (7, 8, 9) THEN 'Q3' ELSE 'Q4' END\").alias(\"quarter_name\"),\n","     expr(\"CASE WHEN dayofweek(date) IN (1,7) THEN 'Weekend' ELSE 'Weekday' END\").alias(\"day_type\"),\n"," )\n","\n"," dim_date_df.write.mode('overwrite').saveAsTable('DimDates')\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"f06bedf1-1a28-4a2b-9697-cf24d681d416","normalized_state":"finished","queued_time":"2025-02-26T23:35:16.261785Z","session_start_time":null,"execution_start_time":"2025-02-26T23:35:16.4310503Z","execution_finish_time":"2025-02-26T23:35:33.1955166Z","parent_msg_id":"1d11a943-6818-4ae6-a44e-c66d93c578d0"},"text/plain":"StatementMeta(, f06bedf1-1a28-4a2b-9697-cf24d681d416, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"171ca83b-bb5f-43f1-a610-cc2a913d8e07"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b35f4e33-71db-4a69-83a5-04535d737440"},{"id":"a00cf91a-f92e-498a-9f14-ba10221fb05a"}],"default_lakehouse":"b35f4e33-71db-4a69-83a5-04535d737440","default_lakehouse_name":"DEV_SLV_LH","default_lakehouse_workspace_id":"177b0c4d-a414-4e32-90f4-a232672971d6"}}},"nbformat":4,"nbformat_minor":5}