{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, when\n","from datetime import datetime\n","import re\n","\n","# Inicia SparkSession si aún no está iniciada\n","spark = SparkSession.builder.appName(\"FileProcessing\").getOrCreate()\n","\n","def get_latest_file(lakehouse_path: str) -> str:\n","    \"\"\"\n","    Retrieves the latest CSV file in a given source table folder based on the filename date.\n","\n","    :param src_table: Name of the table (subfolder in Lakehouse).\n","    :return: Path of the latest file, or None if no valid files are found.\n","    \"\"\"\n","\n","    # Define the base path for the table's folder\n","    # lakehouse_path = f\"abfss://b2c899fb-e571-4496-aebf-c7a23083635a@onelake.dfs.fabric.microsoft.com/a00cf91a-f92e-498a-9f14-ba10221fb05a/Files/User Created Data/{src_table}/\"\n","\n","    try:\n","        # List all files in the specified directory\n","        files_df = spark.read.format(\"binaryFile\").load(lakehouse_path)\n","        file_paths = [row.path for row in files_df.select(\"path\").collect()]\n","        \n","        if not file_paths:\n","            print(f\"No files found in {lakehouse_path}\")\n","            return None\n","\n","        # Define regex pattern for extracting date and table name\n","        pattern = r\".*/(\\d{4}-\\d{2}-\\d{2})([A-Za-z0-9_]+).csv$\"\n","\n","        # Extract dates and table names\n","        valid_files = [\n","            (fp, datetime.strptime(m.group(1), \"%Y-%m-%d\"), m.group(2))\n","            for fp in file_paths\n","            if (m := re.search(pattern, fp))\n","        ]\n","\n","        if not valid_files:\n","            print(f\"No valid files matching pattern found in {lakehouse_path}\")\n","            return None\n","\n","        # Get the latest file based on the extracted date\n","        latest_file = max(valid_files, key=lambda x: x[1])\n","        latest_file_path = latest_file[0]\n","        latest_table_name = latest_file[2]\n","\n","        # Now, read the CSV file into a DataFrame\n","        df = spark.read.csv(latest_file_path, header=True, inferSchema=True)\n","\n","        # Optionally, you can return the DataFrame or just the path\n","        print(f\"Latest file for {lakehouse_path}: {latest_file_path}\")\n","        return df\n","\n","    except Exception as e:\n","        print(f\"Error processing {lakehouse_path}: {e}\")\n","        return None\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"4afccb1a-ad93-4741-837b-dc44c4810f12","normalized_state":"finished","queued_time":"2025-03-25T21:16:28.379369Z","session_start_time":null,"execution_start_time":"2025-03-25T21:16:28.3806927Z","execution_finish_time":"2025-03-25T21:16:28.6689855Z","parent_msg_id":"af158a7a-20e2-46d5-aca4-cd3829f86ed5"},"text/plain":"StatementMeta(, 4afccb1a-ad93-4741-837b-dc44c4810f12, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e6f517dc-1f24-490e-88e2-bf57174d0dec"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"a00cf91a-f92e-498a-9f14-ba10221fb05a","default_lakehouse_name":"LZ_LH_Main","default_lakehouse_workspace_id":"b2c899fb-e571-4496-aebf-c7a23083635a"}}},"nbformat":4,"nbformat_minor":5}